Today, We will be talking about hyperparameters and algorithms which can help us optimize the hyperparameters. The first question which comes to the mind is “How a hyperparameter is different from a parameter?”. Hyperparameters are values that you fix before the learning process starts and they are not changed by the learning process. A few good examples would be batch_size, layer size, the number of layers, learning_rate in the neural nets or regularization weight in the lasso regression.

The next question which pops in mind is “Why hyperparameters need to be optimized?”. The goal of the learning process is to reduce the error on the new data. By tuning the hyperparameters to the optimal values we can get a model that minimizes the error better than any other model having the same learning algorithm. Some algorithms have many hyperparameters and some none. For example, ODE(Ordinary Least Squares) has no hyperparameters while random forest has n_estimators, learning rate, subsample as hyperparameters. Also, different hyperparameters have different effects on the fitting of a model. For example, increasing the learning rate in xgboost makes learning harder & slow and increases chances of overfitting while high values of lambda and alpha which are regularization weights reduce overfitting.

Now comes the part “How do we tune hyperparameters?”. There are different ways to tune hyperparameters like manual tuning(trial & error), grid search, randomized search, evolutionary algorithm e.t.c. We will talk about some of these hyperparameter tuning algorithms and the libraries which implement them. Before we dive into these algorithms and libs we need to choose models(whose hyperparameters are to be optimized), a dataset(for fitting the models) and a cross-validation strategy. We will be using the wine quality dataset and Random Forest & XGBoost as models. Cross-validation is a process to check the trained model’s accuracy on unseen data. We will be using K-Fold cross-validation strategy. In the K-Fold method, a dataset is divided into K parts. Then, we loop over the K parts using ith fold as a test data and train model over the rest of the folds. The accuracy of the model is average accuracy over all the iterations. Let’s read the data and do a bit of cleaning if required. Fire up a bash terminal and run the following commands to download the dataset and install the required libraries.

We will start with Randomized Search algorithm.
In a Randomized search, a new candidate is drawn randomly from the hyperparameter distribution a fixed number of times irrespective of the history of the previously evaluated points. Randomized works better than the grid and manual tuning because not all parameters equally affect the loss function. Unlike GridSearch, RandomizedSearch can be stopped and restarted without any fuss.

Hyperopt: We will be using hyperopt lib’s randomized search algorithm. The salient feature of hyperopt is that it supports many types of parameter search spaces including categorical. For example, If a parameter ‘type’ takes values [‘A’, ‘B’, ‘C’] then you can set up a search space like this ‘hp.choice(‘type’, [‘A’, ‘B’, ‘C’])’. Also, If you wish to return more than the error something like how much time is used by the model to run or whether the model has successfully run or not you can use Trials object. Hyperopt supports both synchronous and parallel operations The interaction point is the fmin function which takes 4 arguments: i) a function to optimize, ii) a search space iii) the maximum number of evaluations to run, and iv) an optional Trial object to return something else than error. Let’s create the parameter search space of both RandomForest and XGBoost models and implement the search function.


Our next algorithm will be Tree-Structured Parzen Estimators.
It a sequential model-based optimization(SMBO) process i.e. it tries to estimate the expected improvement in the loss with a model. Initially, a randomized search algorithm is run for a fixed number of iterations. Then, these points are divided into two groups, best 10-20% as good ones while the rest as the bad ones. At the next iteration, a fixed number of solutions are drawn from the parameter space. Every solution’s expected improvement is measured by the ratio of the probability of being the good one to the probability of being the bad one. Then, the solution with highest expected improvement is tried. The parameter space is divided in form of tree structures. Again, we will use the hyperopt library. Everything will be same as above except algo which is now tpe.suggest.

Next, in line is Bayesian optimization method using Gaussian Processes.
It is a very popular method. When the evaluation of the function is expensive and slow, the function is approximated via a multivariate Gaussian process based on all evaluated points. An acquisition function is chosen which generates a new solution at every iteration by balancing exploitation and exploration. Examples of acquisition functions are Probability of Improvement(PI), Expected Improvement(EI). Using the fitted Gaussian process, a new candidate’s expected improvement is calculated and the candidate with the highest expected improvement is chosen for the next trial. Libraries like spearmint, scikit-optimize provide the implementation of these algorithms. We will use scikit-optimize for our purpose.

Scikit-Optimize: Like hyperopt, scikit-opt also accepts categorical values as parameters. It supports several methods based on sequential model-based optimization including Bayesian optimization using Gaussian processes, sequential optimization using both decision trees and boosted trees. Scikit-opt provides a function gp_minimize which implements the above method. The important parameters of gp_minimize are i) function to minimize, ii) search space, iii) acquisition function, iv) max number of calls.


Now, we will go to Particle Swarm Optimization which is based on swarm intelligence.
PSO is a type of heuristic algorithm based on the social behavior of a group of particles(birds, fish). At the initialization, a fixed number of particles(solutions) are generated randomly. At each iteration, a new position for each particle is generated by taking into account the best position in both global and local search space. For exploration, the global best position is given more weight while in case of exploitation the local best position is more preferred. If the number of particles is large the solution will gravitate towards a randomized search. The important parameters for the PSO algorithm are i) number of particles, ii) the number of generations, iii) weight of local best position, iv) weight of global best position, v) upper bound on how far can be the next position from the current position. We will use Optunity for this purpose.

Optunity: Optunity library supports different approaches and methods like particle swarm optimization(PSO), GridSearchCV, Simplex. It has support for scikit-learn. We will be using  Optunity’s PSO algorithm for hyperparameter optimization. Let’s write the search space an function. We need to remember that solutions generated might be out of bounds. Hence, we need to implement a function to check bounds before the evaluation.


Next, we will talk about LIPO algorithm.
LIPO is a new global optimization method that uses a property of Lipshitz functions to maintain an upper bound on the function. At every iteration, a new solution is randomly drawn from the parameter space and then the function is evaluated if and only if it’s upper bound is better than the current upper bound. It does not have any parameters other than the number of iterations to run. It is important to understand that it will not work reliably if the function is noisy and discontinuous. We will use Dlib library for this method.

Dlib: Dlib is a c++ machine learning and optimization library. It has python support. We will be using an enhanced version of LIPO which is called maxLIPO+TR. Although, the LIPO method is good for getting a global solution it fails to converge the global solution to it’s local best. The Trust-Region(TR) method is added to overcome this shortcoming. The parameter search space is just a list of upper and lower bounds.